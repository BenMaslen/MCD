#Find below some examples of using the image augmentation scripts to augment the Tassie BRUV data set (with 
#surrounding videos) and at the end of script, the Wildcount data set (surrounding images).

#In order for the below examples to run, it is important you follow the below steps first:
#1. Download the annotated image and video data set 'Tassie BRUV' from .... Note that the annotated
#   data is in YOLO txt format. This is currently the only format these scripts can handle.
#2. Install the required packages from requirements.txt into a conda environment.
#3. Ensure the data directory structure is set up as in the data directory from the Tassie BRUV data set,
#   or alter the directory paths in the below code (-l, -t, -ad).
#4. If you are using your own data set, make sure you have created a location_dat file (-l) with all the 
#required fields (see example location_dat files in data/location_data directory):
#   Data set with surrounding videos - 
#       image_name, frame, split, vid_location
#   Data set with surrounding images - 
#       image_name, split, image_folder, image_location

#Let's start by augmenting all the image data using three frame differencing, taking an absolute value approach (-m), with 
#the distance between frames (-d) equal to 1 frame (i.e. we are subtracting neighbouring frames), and we will multiply
#the differences by 15 (-u).

#Let's also block out a time stamp in the top left hand corner of the frames (-s), and resize the video frames to be 
#consistent with the size of the annotated images (-rs). In order to avoid any changes in pixel values based on how 
#the frames were extracted from the video, it can also be a good idea to extract the annotated frames from the video
#(in the same way the neighbouring frames will be extracted) instead of using the saved images in the training data (-t) 
#directory prior to differencing. We can do this using the center command (-c). We will save the augmented frames in a 
#directory titled 'abs_u_15_d_1' (-n).


python -m differencing -l '../../data/Tassie_BRUV/location_data/location_dat.csv'\
 -t '../../data/Tassie_BRUV/training_data/' -rs 1920 1080 -s 0 70 0 635 -c \
 -ad '../../data/Tassie_BRUV/augmented_images/' -n 'abs_u_15_d_1' \
 -d 1 -m 'abs' -u 15

#Now let's change approaches and try a direction based approach to three frame differencing (-m), and only
#replace the red layer with movement information (-r).

python -m differencing -l '../../data/Tassie_BRUV/location_data/location_dat.csv'\
 -t '../../data/Tassie_BRUV/training_data/' -rs 1920 1080 -s 0 70 0 635 -c \
 -ad '../../data/Tassie_BRUV/augmented_images/' -n 'dir_u_15_d_1_r' \
 -d 1 -m 'dir' -u 15 -r

#Let's also try doing background subtraction using KNN from openCV, with a threshold distance of 400 (-td)
#and lead in of 500 (-li). Note we don't need to use -c here, as background subtraction reads in information from the video
#by default. For further details see:
#https://docs.opencv.org/3.4/db/d88/classcv_1_1BackgroundSubtractorKNN.html

python -m background_subtraction -l '../../data/Tassie_BRUV/location_data/location_dat.csv'\
 -t '../../data/Tassie_BRUV/training_data/' -rs 1920 1080 -s 0 70 0 635 \
 -ad '../../data/Tassie_BRUV/augmented_images/' -n 'back_sub_td_400_li_500_r' -r -td 400 -li 500

#Next, let's try reducing the 3 colour layers to two using principal component analysis to attempt to 
#diminish the loss of information that comes with replacing a layer in our annotated frames with
#movement information. In order to do this, we must first estimate principal components that 
#can reduce the three colour layers to two with the below script.

#Here we are extracting 600 frames (-n) from the training set to undergo PCA. Due to you machines memory capabilities,
#you may have to reduce this number. The results are saved as a pca.pkl file in the 'augment directory' (-ad). The 
#script will also save an 'explained_var.csv' file to the the 'augment directory', which details the explained 
#variance from each principal component.

python -m pca_script -l '../../data/Tassie_BRUV/location_data/location_dat.csv'\
 -t '../../data/Tassie_BRUV/training_data/' -c \
 -ad '../../data/Tassie_BRUV/augmented_images/' -n 600

#Now, let's use these principal components with optical flow estimated using the Farneback algorithm in openCV
#(see https://docs.opencv.org/3.4/de/d9e/classcv_1_1FarnebackOpticalFlow.html for further details), to replace the 
#red layer with optical flow (-r) and then dimension reduce the original three colour layers to two, replacing the 
#green and blue layers of the image (-p). The below code looks for the principal components in the augment
#directory (-ad).

python -m optical_flow -l '../../data/Tassie_BRUV/location_data/location_dat.csv'\
 -t '../../data/Tassie_BRUV/training_data/' -rs 1920 1080 -s 0 70 0 635 -c \
 -ad '../../data/Tassie_BRUV/augmented_images/' -n 'flow_1_r_pca' \
 -d 1 -r -p

#Finally let's again use PCA (-r & -p), and estimate background subtraction with a frame differencing based approach.
#To do this, we use the differencing script with method 'diff_BS' (-m). We will again multiply the resulting differences 
#by 15 (-u), and we will take 20 background frames (-bn), each 120 frames away from eachother (-bs).

python -m differencing -l '../../data/Tassie_BRUV/location_data/location_dat.csv'\
 -t '../../data/Tassie_BRUV/training_data/' -rs 1920 1080 -s 0 70 0 635 -c \
 -ad '../../data/Tassie_BRUV/augmented_images/' -n 'diff_BS_bn_20_bs_120_u_15_d_1_r_pca' \
 -d 1 -bn 20 -bs 120 -m 'diff_BS' -u 15 -r -p

#Below are some examples using the Deepfish dataset, which asssumes that the 'image_folder' specified in the location_dat (-l)
#file has surrounding images for the associated annotated image, and the images when ordered alphabetically by their name
#(using the sorted function), specifies the order in which the surrounding burst images were taken. 

#As can be seen the code is the same as above, except we now use differencing_surrounding_images and
#optical_flow_surrounding_images, instead of 'differencing' and 'optical_flow' as before. We also can not undergo background 
#subtraction with KNN as there is generally not enough surrounding images to obtain good estimates (although if you would 
#like to do this, I would recommend converting the surrounding image sequence into videos and using these to estimate KNN
#background subtraction).

python -m differencing_surrounding_images -l '../../data/Deepfish/location_data/location_dat.csv'\
 -t '../../data/Deepfish/training_data/' -c \
 -ad '../../data/Deepfish/augmented_images/' -n 'dir_u_15_d_1_r'  \
 -d 1 -m 'dir' -u 15 -r

python -m optical_flow_surrounding_images -l '../../data/Deepfish/location_data/location_dat.csv'\
 -t '../../data/Deepfish/training_data/' -c \
 -di '../../data/Deepfish/augmented_images/' -n 'flow_1_r_pca' \
 -d 1 -r -p